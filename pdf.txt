InternationalJournalofInnovative
Computing,InformationandControl
ICICInternational
c

2011ISSN1349-4198
Volume
7
,Number
10
,October
2011
pp.
5839{5850
FEED-FORWARDNEURALNETWORKSTRAINING:A
COMPARISONBETWEENGENETICALGORITHMAND
BACK-PROPAGATIONLEARNINGALGORITHM
Zhen-GuoChe
1
,Tzu-AnChiang
2
andZhen-HuaChe
3
;

1
InstituteofInformationManagement
NationalChiaoTungUniversity
No.1001,UniversityRd.,Hsinchu300,Taiwan
bcc102a@hotmail.com
2
DepartmentofBusinessAdministration
NationalTaipeiCollegeofBusiness
No.321,Sec.1,JinanRd.,Taipei100,Taiwan
phdchiang@gmail.com
3
DepartmentofIndustrialEngineeringandManagement
NationalTaipeiUniversityofTechnology
No.1,Sec.3,Chung-HsiaoE.Rd.,Taipei106,Taiwan

Correspondingauthor:zhche@ntut.edu.tw
ReceivedMarch2010;revisedJuly2010
Abstract.
Thisstudydiscussestheadvantagesandcharacteristicsofthegeneticalgo-
rithmandback-propagationneuralnetworktotrainafeed-forwardneuralnetworktocope
withweightingadjustmentproblems.Wecomparetheperformancesofaback-propagation
neuralnetworkandgeneticalgorithminthetrainingoutcomesofthreeexamplesbyre-
ferringtothemeasurementindicatorsandexperimentdata.Theresultsshowthatthe
back-propagationneuralnetworkissuperiortothegeneticalgorithm.Also,theback-
propagationneuralnetworkhasfastertrainingspeedthanthegeneticalgorithm.How-
ever,theback-propagationneuralnetworkhastheshortcomingofovertraining,whilethe
geneticalgorithmdoesnot.Theexperimentresultprovesthattheback-propagationneu-
ralnetworkyieldsbetteroutcomesthanthegeneticalgorithm.
Keywords:
Back-propagationneuralnetwork,Geneticalgorithm,Feed-forwardneural
network
1.
Introduction.
neuralnetworksarebiologicallyinspiredalgo-
rithmsthatconsistofaninputlayerofnodes,oneormorehiddenlayersandanoutput
layer.Eachnodeinalayerhasonecorrespondingnodeinthenextlayer,thuscreatingthe
stacking[1].neuralnetworksaretheveryversatiletoolsandhavebeen
widelyusedtotacklemanyissues[2-6].Feed-forwardneuralnetworks(FNN)areone
ofthepopularstructuresamongneuralnetworks.Thesetnetworksare
widelyusedtosolvecomplexproblemsbymodelingcomplexinput-outputrelationships
[7,8].
However,FNNsoftenendupbeingovertrained.Theyadopttrials-and-errorstoseek
possiblevaluesofparametersforconvergenceoftheglobaloptimum.Thelearningprocess
ofanFNNcannotguaranteetheglobaloptimum,sometimestrappingthenetworkinto
thelocaloptimum.Theback-propagationlearningalgorithm(BPLA)isawidelyused
methodforFNNlearninginmanyapplications.Ithasthegreatadvantageofsimple
implementation[9].Inaddition,manystudieshaveindicatedthatgeneticalgorithms
(GA)canbesuccessfullyappliedtoidentityglobaloptimizationsofmultidimensional
5839
5840
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
functions[10,11].GAsarewidelyappliedintheoptimizationoftheparametersspaces
ofneuralnetworks.Traditionaloptimizationtechniquescanalsodeterminethenumber
ofnetworkparameters,suchasnetworkconnectionweightings;however,theyarenot
abletocontroltheparameteroptimizationsintheabsenceofgradientinformation.In
contrast,GAsareabletoresolvethisissue.Theycanbewidelyusedinallaspectsof
neuralnetworks.Examplesincludethedeterminationofnetworkstructures(thenumber
ofinputnodes,thenumberofhiddenlayers,thenumberofnodesinthehiddenlayers,and
thenumberofoutputnodes),andtheoptimizationofparameterssuchastheselectionof
nodeswitchfunctions,connectionweightingsandlearningspeeds[12].
SextonandGupta[13]pointedoutthatGAsareabletoprocessalargevarietyofdata
types,assoonastheycanbeexpressedwithbitsoflengths.GAsalsoyieldgood
outcomesforoptimization.TheabovestudyprovedthatGAsaresuperiortoBPLAs.
However,theirstudydoesnotspecifythesourcesandquantitiesoftheirdata.Inother
words,theydonotusetdatatypestotrainandcompareaGAandBPLA.
Basedontheaboveanalysis,thisstudyfocusesonperformancesinthetrainingofFNN
withBPLAsandGAs.Thedatatypesusedtocomparetheperformancesofthesetwo
algorithmsareSinfunction,IrisplantsandDiabetes.Thispaperisorganizedasfollows:
Section2introducestheconceptsofBPLAandGA;Section3expressestheprocedures
ofBPLAandGAforFNNtraining;Sections4and5describethedatatypes,resultsand
analyses;,conclusionsaregiveninSection6.
2.
LiteratureReview.
2.1.
Back-propagationneuralnetwork.
BPLAswereproposedbyRumelhartetal.
[14].TheyhavesincebecomefamouslearningalgorithmsamongANNs.Inthelearning
process,toreducetheinaccuracyofANNs,BPLAsusethegradient-decentsearchmethod
toadjusttheconnectionweights.Thestructureofaback-propagationANNisshownin
Figure1.Theoutputofeachneuronistheaggregationofthenumbersofneuronsofthe
previouslevelmultipliedbyitscorrespondingweights.Theinputvaluesareconverted
intooutputsignalswiththecalculationsofactivationfunctions[15].Back-propagation
ANNshavebeenwidelyandsuccessfullyappliedindiverseapplications,suchaspattern
recognition,locationselectionandperformanceevaluations.
Figure1.
Back-propagationANN
FEED-FORWARDNEURALNETWORKSTRAINING
5841
WangandBai[16]appliedaback-propagationANNtoassessandpredictthequalityof
waterbyovercomingproblemsassociatedwithheavyworkloadsandexcessivesubjectivity
intraditionalassessmentmethods.Theresultswereobjectiveandaccurate.Thereisno
needtocarryoutcomplexpre-processinginordertomonitordataandtheworkload
islight.Wuetal.[17]establishedaback-propagationANNfortheidenof
lifespandistributionsofformachineryproducts.Thesimulationresultsshowthatback-
propagationANNsaresuitableforidentifyingthelifespandistributionmodelswhenlarge
rangesofparametersexist.Bongards[18]usedback-propagationtopredictandcontrolthe
improvementindensityofammoniaandtotalnitrogeninwastewaterprocessingplants.
Chouetal.[3]andXiaoandChandrasekar[19]alsosuccessfullyappliedback-propagation
ANNsforpatterningofe-commercecustomerspatterningandrainfallestimationfrom
radardataseparately.Che[20]employedaback-propagationANNforproductandmold
costestimationofplasticinjectionmolding.
2.2.
Geneticalgorithms.
GAswereproposedbyHollandin1975,asoptimization
searchmechanismsfornaturalselection.Thefundamentalprincipleistoimitatethe
lawofnaturalselectioninnaturebychoosingparentswithbettercharacteristics.Ran-
dominteractionsofbitinformationarecarriedout,inthehopeofgenerating
superiortoparents.Thecontinuousrepeatsseethebirthofoptimalspecieswiththe
strongestadaptability.Changetal.[21]appliedaGAandthetestingdesignprinciples
todeveloptheoptimaldesignmodelforundergroundwells.Theoutcomeshowsthatthe
monitoringwellshouldbelocatedintheareaadjacenttothewaterpumpingwelland
thewaterpumpingwellshouldbedeployedintheregionwithsmallertransmission.This
reducestheuncertaintiesofestimationparameters.JiangandYuan[22]proposedcredit
valuationsbasedonGAsandneuralnetworks,andcontrolledtheoccurrenceofType-2
errorsofcommercialbanksthatleadtoheavylossesinthedesignofcreditvaluationsby
usingGAadaptabilityfunctions.Theresultsshowthatthemodelishighinreliabilityand
applicability.JiangandZhai[23]adoptedneuralnetworksandaGAtodesignthinking
patternsofNPCrolesinordertoestablishaself-learningmodelthatismoreautonomous
andintelligentthanthetraditionalNPCroles.Inaddition,GAshavebeenusedtosolve
complicatedproblemsely.Forexample,refertoWuetal.[24],Che[25-27],Che
andWang[28],Chang[29],ShaandChe[30],CheandWang[31]andCheandChaing
[32].
3.
BPLAandGAforFNNTraining.
3.1.
BPLAforFNNtraining.
TheBPLAstructureisbasedmainlyonbatchlearning.
Throughiterationsandmotheaveragesquareerrorsoftestdataareusedto
determinetheoptimalweightsandbiases.TheprocedureofaBPLAisdescribedstepby
stepasfollows:
Step1:
Normalizethedata:Theobtaineddataismappedtothebound[0
;
1],soasto
adjusttherangeofattributesandavoidthesaturationofneurons.The
normalizationofdataiscomputedby
V
new
=(
V
old

MinV
)
=
(
MaxV

MinV
)

(
D
max

D
min
)+
D
min
[33,34].
MinV
istheminimalvalueofthevariables,
MaxV
isthemaximalvalueofthevariables,
D
max
isthemaximumvalueafternormal-
ization,
D
min
istheminimumvalueafternormalization,
V
new
isthenewvalue
afternormalization,and
V
old
istheoldvaluebeforenormalization.
Step2:
Setthenetworkparameters:Numberofhiddenlayers:Ingeneral,oneortwo
hiddenlayersproducebetterconvergence.Toofewortoomanyhiddenlayers
yieldpoorresults[35].Thisstudyusesoneortwohiddenlayers.
Numberofunitsinhiddenlayers:Thelargerthenumberofunitsinthehidden
5842
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
layers,theslowerthespeedofconvergencebecomes.Toosmallofanumberof
unitsinthehiddenlayerisnotttorespondtotheinteractionsbetween
inputvariables.Ontheotherhand,alargenumberofunitsgeneratessmaller
errorbutthecomplexityofthenetworkleadstoslowconvergence[35,36].
Learningrate(

):Ingeneral,toofastortooslowofalearningrateisdetrimental
tonetworkconvergence.Thisstudyselectsvaluesbetween0.1and1.0totestthe
networks[37].
Momentumcot(

):Momentumcotsalsonetworkconver-
gence.Toavoiderrorintheconvergenceprocess,thisstudyselects
thevaluesbetween0.01and1.0[35]totestthenetworks.
Transferfunction:Sigmoidfunction
f
(
x
)=1
=
(1+
e

x
)isthetransferfunction
[38]inthisstudyanditsvalueisintheintervalof[0,1].
Step3:
Inputthedataofthetrainingexamples:Therelatedvaluesofinputandoutput
valuesofthetrainingexamplesareinputtedintotheBPLA.
Step4:
Calculatetheneuron'soutputsignal:Eachneuron'soutputsignaliscalculated
by
net
j
=
P
i
=1
˘
m
w
ji
x
i
+
b
j
,andsigmoidalfunctionisusedtoconvert
net
j
for
eachneuroninthehiddenlayers.
net
j
istheoutputofneuron
j
.
w
ji
istheweight
ontheconnectionfromneuron
i
to
j
;
x
i
istheinputsignaloftheneuron
i
;and
b
i
isthebiasofneuron
j
.
Thesignaloftheoutputlayercanexpressedby
net
k
=
TV
k
+

L
k
.
TV
k
isthe
targetvalueoftheoutputneuron
k
and

L
k
istheerrorofneuron
k
.Theerrorof
eachhiddenlayeris

l
j
=
P
i
=1
˘
I

l
+1
i
w
l
ji
f
0
(
net
l
j
)andtheincrementalchangefor
everyweightforeachlearninginteractioniscomputedby
w
l
ji
=

l
+1
j
f
0
(
net
l
j
)+


w
l

1
ji
.
f
0
(
:
)isthederivativeofthesigmoidfunction.
Step5:
Calculatetheerrorvalues:Steps3-5arerepeateduntilthenetworkisconverged
[35].Theerroriscalculatedby
SSE
=
P
i
=1
˘
n
(
T
i

Y
i
)
2
.
T
i
istheactualvalue
and
Y
i
istheestimatedvalue.
3.2.
GAforFNNtraining.
ComputationstepsoftheGAareasfollows:
Step1:
Randomlygenerateinitialpopulation:
N
chromosomesaregeneratedrandomly
inordertoserveastheinitialpopulationintheevolutionaryprocess.Theirgenes
areofrealnumbersinvalueandaregeneratedrandomly,withthevaluesranging
between0
:
3and

0
:
3[39].
Step2:
Calculatethevalues:Fitnessfunctionissetuptoperformtheevolutionary
process.Thefunctionoftheexperimentisfeed-forwardcalculationforthe
computationofSSE.
Step3:
Selecttheindividuals:ThesmallertheSSE,thehighertheprobabilityofthe
chromosomesbeingpasseddowntothenextgeneration[40].Calculatethe
functionvalue
f
i
foreachindividualinthepopulationandtheaggregationofall
thefunctionvalues
f
sum
.Arandomnumber
r
rand
isselectedintherange
of[0
;f
min
].
r
rand
isdeductedbythefunctionvalueinthepopulation:
r
rand
=
r
rand

f
i
.Thisisrepeateduntilthevalueof
r
rand
minus
f
k
issmalleror
equaltozero.The
k
th
individualwillbeduplicatedintothematingpool.The
selectionisrepeateduntilthenumberofindividualsinthematingpoolisthe
sameasthenumberofindividualsinthepopulation.
Step4:
Crossindividuals:Twoindividualsareselectedrandomlyfromthematingpool
astheparentandtherandomselectionofageneisusedasacrossoverpoint.The
exchangeoftheparentgenestotherightofthecrossoverpointgeneratestwonew
individuals,whichmarkstheendofthesingle-pointcrossover[41].
FEED-FORWARDNEURALNETWORKSTRAINING
5843
Step5:
Mutategene:Theapproximateoptimalsolutionscanbefoundquicklyinorder
tosetupthemutationrateasaparametertocontrolmutationprobability.The
mutationrateisarealnumberbetween0and1[42].
Step6:
Judgetheterminationconditions:Thestopconditionissetasthegeneration
number.Ifthesetgenerationnumberofisreached,itwillstop.
4.
DataDescription.
Thisstudyworksonthreedatatypes,Sinfunction,Irisand
Diabetes.Thesedatatypesaredescribedasfollows:
Sinfunction:Thereareatotalof315Sinfunctiondataentries.Kumar[35]divideddata
intothedatatype60%-20%-20%,189trainingentries,63testentriesand63validation
entries.Theinputandoutputareseparateneurons.Thenetworkstructureisillustrated
inFigure2.TheconstantmoofweightsinthetrainingofBPLAenablethe
derivedcurvetosimulatetheoriginallines.
Figure2.
NetworkstructureforSinfunction
Irisplants:Thereareatotalof150dataentries.Dataisdividedintothefollowing
types:60%-20%-20%,90trainingentries,30testentriesand30validationentries.The
datarecordsthecharacteristicsofiriswers.Thereare4attributesand3
These4attributesareusedtopredictwhichofthethreetypesacertainwerbelongs
to.ThenetworkstructureisillustratedinFigure3.Thethreetypesofiriswersare
IrisSetosa,IrisVersicolourandIrisVirginica.Thefourattributesaresepallength,sepal
width,petallengthandpetalwidth.
Diabetes:Familydiabetesisdeterminedaccordingtothe
UCIMachineLearningRepos-
itory
'schosendata.Atotalof8attributes(numbersofpregnancy,bloodsugarlevels,
diastolicbloodpressure,subcutaneousfatthicknessofbrachialtricepsmuscle,serumin-
sulinlevels,BodyMassIndex,familydiabeteshistoryandages)areusedtodetermine
twotypes.Thereareatotalof768dataentries,460trainingentries,154testentries
and154validationentries.ThenetworkstructureisillustratedinFigure4.Therefore,
eightinputneuronsaredesignedtorepresenttattributesandtwooutputneurons.
Theyrepresentthepresenceorabsenceofhereditarydiabetes.
TherelatedparametersettingsintheBPLAandGAare:learningrate=0
:
25,0.5,
0.9;momentum=0
:
01,0.5,0.9;populationsize=20;crossoverrate=0
:
7,0.8,0.9;
mutationrate=0
:
01,0.02,0.05;iterations/generations=1000,2000,3000,10000,20000,
30000.
5844
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
Figure3.
NetworkstructureforIrisplants
Figure4.
NetworkstructureforDiabetes
5.
ResultsandAnalyses.
Aftersearchingforthebestparameters,asdescribedabove,
thisstudyusestwoperformanceindicators,meansquareerror(MSE)andCPUtime,to
verifytheperformancesoftheBPLAandGA.MSEistheevaluationindex[14,40]and
itsformulaisexpressedas
MSE
=
p
P
i
=1
˘
n
(
T
i

E
i
)
2

n
.Wheniterations/generations
reachthesetmaximumiterations/generations,thealgorithmterminatesthetraining.
Eachalgorithmisrunetimeswiththesamenumberofhiddenlayers.
Table1.
ExperimentsoflearningrateandmomentumfortheBPLAinSinfunction
Iterations
Learningrate
0.9
0.5
0.25
Momentum
0.01
0.5
0.9
0.01
0.5
0.9
0.01
0.5
0.9
1000
TrainingMSE
0.0262
0.1397
0.1702
0.0471
0.1653
0.1625
0.1288
0.1641
0.1773
TestingMSE
0.1000
0.1510
0.1730
0.1005
0.1684
0.1670
0.1455
0.1677
0.1791
2000
TrainingMSE
0.0235
0.1508
0.1672
0.0500
0.1608
0.1619
0.1283
0.1520
0.1741
TestingMSE
0.1001
0.1616
0.1704
0.1025
0.1651
0.1659
0.1450
0.1614
0.1763
3000
TrainingMSE
0.0237
0.1337
0.1611
0.0585
0.1584
0.1559
0.1229
0.1568
0.1663
TestingMSE
0.1000
0.1474
0.1657
0.1075
0.1635
0.1617
0.1421
0.1624
0.1697
FEED-FORWARDNEURALNETWORKSTRAINING
5845
Table2.
ExperimentsofiterationsfortheBPLAinSinfunction
Iterations
1000
2000
3000
10000
20000
30000
TrainingMSE
0.0262
0.0235
0.0237
0.0236
0.0235
0.0238
TestingMSE
0.1000
0.1000
0.1000
0.1000
0.1001
0.1000
Table3.
ExperimentsofcrossoverrateandmutationratefortheGAin
Sinfunction
Generations
Crossover
0.7
0.8
0.9
Mutation
0.01
0.02
0.05
0.01
0.02
0.05
0.01
0.02
0.05
1000
TrainingMSE
0.0159
0.0245
0.0523
0.0233
0.0261
0.0464
0.0196
0.0253
0.0493
TestingMSE
0.1479
0.1467
0.1252
0.1473
0.1521
0.1288
0.1476
0.1494
0.1321
2000
TrainingMSE
0.0155
0.0238
0.0528
0.0217
0.0271
0.0412
0.0188
0.0214
0.0418
TestingMSE
0.1471
0.1433
0.1255
0.1502
0.1733
0.1206
0.1409
0.1503
0.1433
3000
TrainingMSE
0.0153
0.0239
0.0644
0.0211
0.0267
0.0432
0.0192
0.0233
0.0455
TestingMSE
0.1396
0.1477
0.1345
0.1498
0.1676
0.1308
0.1434
0.1501
0.1332
Table4.
ExperimentsofgenerationsfortheGAinSinfunction
Generations
1000
2000
3000
10000
20000
30000
TrainingMSE
0.0158
0.0154
0.0153
0.0159
0.0153
0.0160
TestingMSE
0.1479
0.1470
0.1396
0.1480
0.1415
0.1513
Table5.
PerformancecomparisonsbetweentheBPLAandGAinSinfunction
Hiddennumber
BPLA
GA
Hiddennumber
BPLA
GA
S1
MSE
Time(s)
MSE
Time(s)
S1
MSE
Time(s)
MSE
Time(s)
4
0.0262
3.08
0.0159
8.86
10
0.0241
4.57
0.0250
7.88
5
0.0204
3.53
0.0153
8.34
11
0.0212
4.55
0.0221
7.42
6
0.0143
3.96
0.0184
8.53
13
0.0188
6.75
0.0207
8.65
7
0.0192
4.32
0.0192
7.38
15
0.0160
7.83
0.0188
9.78
8
0.0256
6.53
0.0209
6.76
20
0.0182
8.23
0.0182
8.35
9
0.0243
7.78
0.0256
7.42
30
0.0185
8.98
0.0198
11.93
5.1.
Sinfunction
Tables1and2showthatwhenlearningrate=0
:
9,momentum=
0
:
01andmaximumiterations=2000,theMSEisminimizedfortrainingandtestingin
theback-propagationANN.Similarly,inTables3and4,asthebestparametersinthe
GAreachcrossoverrate=0
:
7,mutationrate=0
:
01andgenerations=3000,theMSE
isminimized,theaveragesareaggregatedandcompared,asshowninTable5.Table5
indicatesthattheBPLAistlysuperiortotheGA.Giventhesamenumberof
neurons,theCPUtimetoruntheBPLAisshorterthanthattoruntheGA.Intermsof
errorperformancemeasurement,theBPLAreportslowererrorsthantheGA.
5.2.
Irisplant
TheofIRISappliesaPlantsDatabaseto
traintheFNN.ThebestparametersintheBPLAare:iterations=1000,learningrate=
0
:
5,momentum=0
:
01andthoseinGAare:generations=2000,crossoverrate=0
:
7,
mutation=0
:
02.TheresultsareshowninTables6-9.Table10showsthecomparisons
betweentheBPLAandGAinregardtoMSEandCPUtimeandindicatesthattheGA
issuperiortotheBPLAinMSE,buttheGArequireslongerCPUtimethantheBPLA.
5.3.
Diabetesdetermination.
ThebestparametersinthatBPLAareasfollows:learni-
ngrates=0
:
9,momentumcots=0
:
01anditerations=20000(Tables11and12).
ThebestparametersinGAare:crossoverrate=0
:
7,mutation=0
:
02,generations=
3000(Tables13and14).AsshowninTable15,giventhesamenumberofneurons,the
5846
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
Table6.
ExperimentsoflearningrateandmomentumfortheBPLAinIrisplants
Iterations
Learningrate
0.9
0.5
0.25
Momentum
0.01
0.5
0.9
0.01
0.5
0.9
0.01
0.5
0.9
1000
TrainingMSE
0.0620
0.0446
0.0566
0.0398
0.0615
0.0532
0.0556
0.0681
0.0544
TestingMSE
0.0682
0.0568
0.0685
0.0540
0.0748
0.0677
0.0628
0.0756
0.0681
2000
TrainingMSE
0.0571
0.0501
0.0568
0.0406
0.0616
0.0505
0.0515
0.0596
0.0537
TestingMSE
0.0681
0.0577
0.0683
0.0549
0.0755
0.0664
0.0606
0.0724
0.0675
3000
TrainingMSE
0.0566
0.0504
0.0519
0.0470
0.0612
0.0580
0.0574
0.0593
0.0554
TestingMSE
0.0685
0.0655
0.0688
0.0575
0.0695
0.0641
0.0637
0.0714
0.0665
Table7.
ExperimentsofiterationsfortheIrisplants
Iterations
1000
2000
3000
10000
20000
30000
Evaluationindices
TrainingMSE
0.0398
0.0406
0.0470
0.0453
0.0412
0.0409
TestingMSE
0.0504
0.0549
0.0575
0.0517
0.0522
0.0523
Table8.
ExperimentsofcrossoverrateandmutationratefortheGAin
Irisplants
Generations
Crossover
0.7
0.8
0.9
Mutation
0.01
0.02
0.05
0.01
0.02
0.05
0.01
0.02
0.05
1000
TrainingMSE
0.0608
0.0394
0.0554
0.0602
0.0438
0.0509
0.0548
0.0667
0.0533
TestingMSE
0.0668
0.0529
0.0671
0.0733
0.0558
0.0663
0.0615
0.0741
0.0667
2000
TrainingMSE
0.0559
0.0309
0.0556
0.0603
0.0491
0.0494
0.0501
0.0584
0.0526
TestingMSE
0.0665
0.0418
0.0669
0.0735
0.0565
0.0650
0.0593
0.0709
0.0661
3000
TrainingMSE
0.0556
0.0460
0.0508
0.0599
0.0494
0.0568
0.0563
0.0578
0.0543
TestingMSE
0.0672
0.0564
0.0666
0.0681
0.0644
0.0628
0.0617
0.0695
0.0652
Table9.
ExperimentsofgenerationsfortheGAinIrisplants
Generations
1000
2000
3000
10000
20000
30000
TrainingMSE
0.0394
0.0309
0.0388
0.0322
0.0311
0.0330
TestingMSE
0.0528
0.0418
0.0516
0.0467
0.0447
0.0465
Table10.
PerformancecomparisonsbetweentheBPLAandGAinIrisplants
Hiddennumber
BPLA
GA
S1
MSE
Time(s)
MSE
Time(s)
4
0.0398
2.30
0.0309
4.40
6
0.0318
3.30
0.0298
6.82
8
0.0311
4.60
0.0293
8.87
10
0.3540
5.71
0.0315
12.32
12
0.3090
16.42
0.0306
18.80
14
0.4050
10.88
0.0319
26.70
16
0.3600
19.10
0.0311
38.30
Table11.
ExperimentsoflearningrateandmomentumfortheBPLAinDiabetes
Iterations
Learningrate
0.9
0.5
0.25
Momentum
0.01
0.5
0.9
0.01
0.5
0.9
0.01
0.5
0.9
1000
TrainingMSE
0.0401
0.0632
0.0905
0.04165
0.0406
0.0426
0.0432
0.0519
0.0667
TestingMSE
0.4816
0.7123
0.5143
0.5124
0.4895
0.5390
0.5432
0.6009
0.5266
2000
TrainingMSE
0.0388
0.0583
0.0760
0.0402
0.0403
0.0463
0.0416
0.0493
0.0611
TestingMSE
0.4789
0.6834
0.5092
0.5011
0.4823
0.5452
0.5233
0.5829
0.5272
3000
TrainingMSE
0.0363
0.0443
0.0708
0.0369
0.0399
0.0415
0.0375
0.0421
0.0561
TestingMSE
0.4673
0.5437
0.5172
0.4830
0.4808
0.5157
0.4987
0.5123
0.5164
FEED-FORWARDNEURALNETWORKSTRAINING
5847
Table12.
ExperimentsofiterationsfortheBPLAinDiabetes
Iterations
1000
2000
3000
10000
20000
30000
TrainingMSE
0.0401
0.0388
0.0363
0.0319
0.0189
0.0202
TestingMSE
0.4816
0.4789
0.4673
0.4208
0.3192
0.3674
Table13.
ExperimentsofcrossoverrateandmutationratefortheGAinDiabetes
Generations
Crossover
0.7
0.8
0.9
Mutation
0.01
0.02
0.05
0.01
0.02
0.05
0.01
0.02
0.05
1000
TrainingMSE
0.0471
0.0248
0.0399
0.0557
0.0354
0.0413
0.0424
0.0383
0.0403
TestingMSE
0.5682
0.3803
0.5511
0.6120
0.5772
0.7123
0.5617
0.5432
0.5524
2000
TrainingMSE
0.0289
0.0286
0.0339
0.0351
0.0357
0.0343
0.0301
0.0436
0.0369
TestingMSE
0.5599
0.4272
0.5356
0.5752
0.6106
0.5227
0.5351
0.7321
0.5636
3000
TrainingMSE
0.0218
0.0229
0.0334
0.0387
0.0404
0.0367
0.0298
0.0405
0.0352
TestingMSE
0.5363
0.3806
0.0533
0.6431
0.7086
0.6675
0.4636
0.7096
0.5776
Table14.
ExperimentsofgenerationsfortheGAinDiabetes
Generations
1000
2000
3000
10000
20000
30000
TrainingMSE
0.0248
0.0285
0.0229
0.0285
0.0286
0.0304
TestingMSE
0.3802
0.4272
0.3805
0.4234
0.4335
0.4735
Table15.
PerformancecomparisonsbetweentheBPLAandGAinDiabetes
Hiddennumber
BPLA
GA
S1
MSE
Time(s)
MSE
Time(s)
4
0.33289
20.20000
0.53652
33.70000
5
0.31927
22.10000
0.38058
37.30000
6
0.28892
26.30000
0.37654
40.60000
7
0.32234
27.00000
0.30134
44.70000
8
0.30068
36.80000
0.29842
49.90000
CPUtimetoruntheBPLAisshorterthanthattoruntheGA.IntermsofMSE,the
BPLAreportsfewererrorsthantheGA.
6.
Conclusions.
ThisstudyfocusesontrainingaFNNbyusingaBPLAandGA.To
comparetheperformancesoftheBPLAandGA,threedatasets,Sinfunction,Irisplants
andDiabetes,areemployedinthisstudy.Accordingtothemeasurementindicators,the
BPLAisindeedslightlysuperiortotheGAundercertainconditions.Also,theBPLA
isfasterthantheGAintermsoftrainingspeeds.However,theBPLAhastheproblem
ofovertraining,whereas,theGAdoesnot.IntermsofCPUtimerequired,theBPLA
yieldsbetterresultsthantheGA.Asfarastheoftlearningratesand
momentumcotsonBPLAconvergence,theexperimentsshowthatlearningrates
thespeedofconvergence.Also,whenthemomentumcotsaresmall,
theyalsotheconvergencespeedofMSE.Whenthelearningrateishighandthe
momentumcotislarge,MSEwilloverlyHowever,whenbothparameters
arenottoohigh,theMSEconvergencedoesnotNeuralnetworkshavelearning
andfault-tolerancecharacteristics.GAshavethecapabilitiestothebestsolutionsin
globalsearches.Thisstudythatintheprocessofseekingsolutions,back-propagation
ANNsreportmorestableconvergenceofbestsolutions,butrequirealargernumberof
learningcycles.Ontheotherhand,GAsrequireasmallernumberoflearningcycles
tothebestsolutions,butneedlongertrainingtime.ToenhancetheFNNtraining
5848
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
quality,thecurrentmethods,BPLAandGA,willbeimprovedbyintroducingthelocal
searchmechanisminourfurtherstudies.
Acknowledgement.
ThisworkispartiallysupportedbytheNationalScienceCouncil
underprojectsNo.NSC98-2410-H-027-002-MY2andNo.NSC99-2622-H-027-001-CC3.
Theauthorsalsogratefullyacknowledgethehelpfulcommentsandsuggestionsofthe
Editorsandreviewers,whichhaveimprovedthepresentation.
REFERENCES
[1]
S.ShrivastavaandM.P.Singh,Performanceevaluationoffeed-forwardneuralnetworkwithsoft
computingtechniquesforhandwrittenEnglishalphabets,
AppliedSoftComputing
,vol.11,no.1,
pp.1156-1182,2011.
[2]
Y.E.ShaoandB.-S.Hsu,DeterminingthecontributorsforamultivariateSPCchartsignalusingar-
neuralnetworksandsupportvectormachine,
InternationalJournalofInnovativeComputing,
InformationandControl
,vol.5,no.12(B),pp.4899-4906,2009.
[3]
P.-H.Chou,C.-H.Hsu,C.-F.Wu,P.-H.LiandM.-J.Wu,Applicationofback-propagationneural
networkfore-commercecustomerspatterning,
ICICExpressLetters
,vol.3,no.3(B),pp.775-785,
2009.
[4]
C.He,H.Li,B.Wang,W.YuandX.Liang,Predictionofcompressiveyieldloadformetalhollow
spherewithcrackbasedonneuralnetwork,
ICICExpressLetters
,vol.3,no.4(B),pp.1263-
1268,2009.
[5]
J.K.Wu,J.Kang,M.H.ChenandG.T.Chen,Fuzzyneuralnetworkmodelbasedonparticle
swarmoptimizationforshort-termloadforecasting,
Proc.oftheCSU-EPSA
,vol.19,no.1,pp.63-67,
2007.
[6]
D.K.Li,H.X.ZhangandS.A.Li,DevelopmentcostestimationofaircraftframebasedonBP
neuralnetworks,
FireControlandCommandControl
,vol.31,no.9,pp.27-29,2006.
[7]
B.Karimi,M.B.MenhajandI.Saboori,Multilayerfeedforwardneuralnetworksforcontrollingde-
centralizedlarge-scalenonlinearsystemswithguaranteedstability,
InternationalJournal
ofInnovativeComputing,InformationandControl
,vol.6,no.11,pp.4825-4841,2010.
[8]
B.ZareNezhadandA.Aminian,Amulti-layerfeedforwardneuralnetworkmodelforaccurate
predictionofgassulfuricaciddewpointsinprocessindustries,
AppliedThermalEngineering
,
vol.30,no.6-7,pp.692-696,2010.
[9]
B.Choi,J.H.LeeandD.H.Kim,Solvinglocalminimaproblemwithlargenumberofhiddennodes
ontwo-layeredfeed-forwardneuralnetworks,
Neurocomputing
,vol.71,no.16-18,pp.3640-
3643,2008.
[10]
H.S.ChungandJ.J.Alonso,Multiobjectiveoptimizationusingapproximationmodel-basedgenetic
algorithms,
The10thAIAA/ISSMOMultidisciplinaryAnalysisandOptimizationConference
,vol.1,
pp.275-291,2004.
[11]
B.Chu,D.Kim,D.Hong,J.Park,J.T.Chung,J.-H.ChungandT.H.Kim,GA-basedfuzzy
controllerdesignfortunnelventilationsystems,
AutomationinConstruction
,vol.17,no.2,pp.130-
136,2008.
[12]
M.L.HuangandY.H.Hung,Combiningradialbasisfunctionneuralnetworkandgeneticalgorithm
toimproveHDDdriverICchipscalepackageassemblyyield,
ExpertSystemswithApplications
,
vol.34,no.1,pp.588-595,2008.
[13]
R.S.SextonandJ.N.D.Gupta,Comparativeevaluationofgeneticalgorithmandbackpropagation
fortrainingneuralnetworks,
InformationSciences
,vol.129,no.1-4,pp.45-59,2000.
[14]
D.E.Rumelhart,G.E.HintonandR.J.Williams,Learningrepresentationsbyback-propagating
errors,
Nature
,vol.323,no.6088,pp.533-536,1986.
[15]
M.N.HajmeerandI.A.Basheer,Ahybridbayesian-neuralnetworkapproachforprobabilistic
modelingofbacterialgrowth/no-growthinterface,
InternationalJournalofFoodMicrobiology
,vol.82,
no.3,pp.233-243,2003.
[16]
Y.Q.WangandX.Q.Bai,PredictionandevaluationofwaterqualityusingBPneuralnetwork,
JournalofWuhanPolytechnicUniversity
,vol.26,no.1,pp.64-67,2007.
[17]
Y.M.Wu,Y.Q.WangandL.Li,ApplicationstudyonBPnetworkandgeneralizedRBFnetworkin
estimatingdistributionmodelofmechanicalproducts,
ChinaMechanicalEngineering
,vol.17,no.20,
pp.2140-2144,2006.
FEED-FORWARDNEURALNETWORKSTRAINING
5849
[18]
M.Bongards,Improvingtheofawastewatertreatmentplantbyfuzzycontrolandneural
networks,
WaterScienceandTechnology
,vol.43,no.11,pp.189-196,2001.
[19]
R.XiaoandV.Chandrasekar,Developmentofaneuralnetworkbasedalgorithmforrainfalles-
timationfromradarobservations,
IEEETransactionsonGeoscienceandRemoteSensing
,vol.35,
pp.160-171,1997.
[20]
Z.-H.Che,PSO-basedback-propagationneuralnetworkforproductandmoldcostestima-
tionofplasticinjectionmolding,
ComputersandIndustrialEngineering
,vol.58,no.4,pp.625-637,
2010.
[21]
L.C.Chang,H.J.Chu,Y.W.ChenandH.H.Chen,Applicationofgeneticalgorithmandexperi-
mentaldesignondesigningagroundwaterwellnetwork,
JournalofChineseAgriculturalEngineering
,
vol.52,no.2,pp.1-11,2006.
[22]
M.H.JiangandX.C.Yuan,ConstructionandapplicationofpersonalcreditassessmentGAneural
networkmodel,
JournalofWuhanUniversityofScienceandTechnology(SocialScienceEdition)
,
vol.9,no.4,pp.368-372,2007.
[23]
N.JiangandY.Q.Zhai,Gamedesignofselfautomationbasedonneuralnetsandgenetic
algorithms,
JournalofComputerApplications
,vol.27,no.5,pp.1280-1282,2007.
[24]
Z.Y.Wu,Z.ZhangandQ.Hu,Theoptimizationforconceptualdesignofhigh-risebuildingsbased
onmulti-objectivegeneticalgorithm,
SystemsEngineering{TheoryandPractice
,vol.25,no.10,
pp.121-124,2005.
[25]
Z.-H.Che,Usinghybridgeneticalgorithmsformulti-periodproductchangeplanning,
InternationalJournalofInnovativeComputing,InformationandControl
,vol.6,no.6,pp.2761-2785,
2010.
[26]
Z.-H.Che,Atwo-phasehybridapproachtosupplierselectionthroughclusteranalysiswithmultiple
dimensions,
InternationalJournalofInnovativeComputing,InformationandControl
,vol.6,no.9,
pp.4093-4111,2010.
[27]
Z.-H.Che,Ageneticalgorithm-basedmodelforsolvingmulti-periodsupplierselectionproblemwith
assemblysequence,
InternationalJournalofProductionResearch
,vol.48,no.15,pp.4355-4377,2010.
[28]
Z.-H.CheandH.-S.Wang,Supplierselectionandsupplyquantityallocationofcommonandnon-
commonpartswithmultiplecriteriaundermultipleproducts,
ComputersandIndustrialEngineering
,
vol.55,no.1,pp.110-133,2008.
[29]
J.-F.Chang,Aperformancecomparisonbetweengeneticalgorithmsandparticleswarmoptimization
appliedinconstructingequityportfolios,
InternationalJournalofInnovativeComputing,Informa-
tionandControl
,vol.5,no.12(B),pp.5069-5079,2009.
[30]
D.Y.ShaandZ.-H.Che,Supplychainnetworkdesign:partnerselectionandproduction/distribution
planningusingasystematicmodel,
JournaloftheOperationalResearchSociety
,vol.57,no.1,pp.52-
62,2006.
[31]
Z.-H.CheandH.-S.Wang,Ahybridapproachforsupplierclusteranalysis,
ComputersandMathe-
maticswithApplications
,vol.59,no.2,pp.745-763,2010.
[32]
Z.-H.CheandC.-J.Chiang,Amoparetogeneticalgorithmformulti-objectivebuild-to-order
supplychainplanningwithproductassembly,
AdvancesinEngineeringSoftware
,vol.41,no.7-8,
pp.1011-1022,2010.
[33]
M.H.Hassoun,
FundamentalsofANeuralNetworks
,MITPress,Cambridge,London,MA,
1995.
[34]
T.Masters,
PracticalNeuralNetworkRecipesinC++
,AcademicPressProfessional,Inc.,SanDiego,
CA,1993.
[35]
S.Kumar,
NeuralNetworks
,McGraw-Hill,NewYork,2005.
[36]
M.Li,X.Guo,X.D.TanandJ.H.Yuan,Predictivemethodforpowergrowthbasedonimproved
BPneuralnetworkandrebuildingofnewindustrystructure,
JournalofCentralSouthUniversity
(ScienceandTechnology)
,vol.38,no.1,pp.143-147,2007.
[37]
L.Fu,
NeuralNetworksinComputerIntelligence
,McGraw-Hill,NewYork,1995.
[38]
E.E.Tabach,L.Lancelot,I.ShahrourandY.Najjar,Useofnetworksimulationmetamod-
ellingtoassessgroundwatercontaminationinaroadproject,
MathematicalandComputerModelling
,
vol.45,no.7-8,pp.766-776,2007.
[39]
E.ZhouandA.Khotanzad,Fuzzydesignusinggeneticalgorithms,
PatternRecognition
,
vol.40,no.12,pp.3401-3414,2007.
[40]
K.W.Chau,ApplicationofPSO-basedneuralnetworkinanalysisofoutcomesofconstruction
claims,
AutomationinConstruction
,vol.16,no.5,pp.642-646,2006.
5850
Z.-G.CHE,T.-A.CHIANGANDZ.-H.CHE
[41]
A.Meng,L.Ye,D.RoyandP.Padilla,Geneticalgorithmbasedmulti-agentsystemappliedtotest
generation,
ComputersandEducation
,vol.49,no.4,pp.1205-1223,2007.
[42]
E.Eiben,R.HinterdingandZ.Michalewicz,Parametercontrolinevolutionaryalgorithms,
IEEE
TransactionsonEvolutionaryComputation
,vol.3,pp.124-141,1999.
